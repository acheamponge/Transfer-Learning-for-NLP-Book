{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 517401 rows and 2 columns\n",
      "                       file                                            message\n",
      "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
      "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
      "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
      "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
      "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "filepath = \"./data/emails.csv\"\n",
    "\n",
    "emails = pd.read_csv(filepath)\n",
    "\n",
    "print(\"Successfully loaded {} rows and {} columns\".format(emails.shape[0],emails.shape[1]))\n",
    "print(emails.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(emails.loc[0][\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "\n",
    "def extract_messages(df):\n",
    "    messages=[]\n",
    "    for item in df[\"message\"]:\n",
    "        e=email.message_from_string(item)\n",
    "        message_body=e.get_payload()\n",
    "        messages.append(message_body)\n",
    "    print(\"Successfully retrieved message body from emails\")\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved message body from emails\n"
     ]
    }
   ],
   "source": [
    "bodies = extract_messages(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies_df = pd.DataFrame(bodies)\n",
    "#bodies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 3978 spam emails!\n"
     ]
    }
   ],
   "source": [
    "filepath2 = \"./data/fradulent_emails.txt\"\n",
    "with open(filepath2,'r',encoding=\"latin1\") as file:\n",
    "    data=file.read()\n",
    "    \n",
    "fraud_emails = data.split(\"From r\")\n",
    "print(\"Successfully loaded {} spam emails!\".format(len(fraud_emails)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved message body from emails\n"
     ]
    }
   ],
   "source": [
    "fraud_bodies = extract_messages(pd.DataFrame(fraud_emails, columns=[\"message\"],dtype=str))\n",
    "fraud_bodies_df=pd.DataFrame(fraud_bodies[1:])\n",
    "#print(fraud_bodies_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsamp=1000\n",
    "maxtokens=50\n",
    "maxtokenlen=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(row):\n",
    "    if row in [None,'']:\n",
    "        tokens=\"\"\n",
    "    else:\n",
    "        tokens=str(row).split(\" \")[:maxtokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def reg_expressions(row):\n",
    "    tokens=[]\n",
    "    try:\n",
    "        for token in row:\n",
    "            token=token.lower()\n",
    "            token=re.sub(r'[\\W\\d]', \"\", token)\n",
    "            token=token[:maxtokenlen]\n",
    "            tokens.append(token)\n",
    "    except:\n",
    "        token=\"\"\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopword: Package 'stopword' not found in\n",
      "[nltk_data]     index\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopword')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "def stop_words_removal(row):\n",
    "    token = [token for token in row if token not in stopwords]\n",
    "    token = filter(None, token)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "EnronEmails = bodies_df.iloc[:,0].apply(tokenize)\n",
    "EnronEmails = EnronEmails.apply(stop_words_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnronEmails = EnronEmails.apply(reg_expressions)\n",
    "EnronEmails = EnronEmails.sample(Nsamp)\n",
    "\n",
    "SpamEmails = fraud_bodies_df.iloc[:,0].apply(tokenize)\n",
    "SpamEmails = SpamEmails.apply(stop_words_removal)\n",
    "SpamEmails = SpamEmails.apply(reg_expressions)\n",
    "SpamEmails = SpamEmails.sample(Nsamp)\n",
    "\n",
    "raw_data = pd.concat([SpamEmails,EnronEmails], axis=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of combined data represented as NumPy array is:\n",
      "(2000,)\n",
      "Data represented as NumPy array is:\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of combined data represented as NumPy array is:\")\n",
    "print(raw_data.shape)\n",
    "print(\"Data represented as NumPy array is:\")\n",
    "#print(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Categories = ['spam', 'notspam']\n",
    "header = ([1]*Nsamp)\n",
    "header.extend(([0]*Nsamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_bag(data):\n",
    "    used_tokens=[]\n",
    "    all_tokens=[]\n",
    "    \n",
    "    for item in data:\n",
    "        for token in item:\n",
    "            if token in all_tokens:\n",
    "                if token not in used_tokens:\n",
    "                    used_tokens.append(token)\n",
    "            else:\n",
    "                all_tokens.append(token)\n",
    "    df=pd.DataFrame(0, index=np.arange(len(data)), columns=used_tokens)\n",
    "    \n",
    "    for i, item in enumerate(data):\n",
    "        for token in item:\n",
    "            if token in used_tokens:\n",
    "                df.iloc[i][token] += 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnronSpamBag = assemble_bag(raw_data)\n",
    "#EnronSpamBag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors=[column for column in EnronSpamBag.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffle_data(data,header):\n",
    "    p=np.random.permutation(len(header))\n",
    "    data=data[p]\n",
    "    header=np.asarray(header)[p]\n",
    "    return data,header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, header = unison_shuffle_data(EnronSpamBag.values, header)\n",
    "idx = int(0.7*data.shape[0])\n",
    "train_x = data[:idx]\n",
    "train_y = header[:idx]\n",
    "test_x = data[:idx]\n",
    "test_y = header[:idx:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
