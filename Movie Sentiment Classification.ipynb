{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "!tar xzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    data, sentiments=[], []\n",
    "    for folder, sentiment in (('neg',0),('pos',1)):\n",
    "        folder = os.path.join(path,folder)\n",
    "        for name in os.listdir(folder):\n",
    "            with open(os.path.join(folder,name),'r',encoding=\"utf8\") as reader:\n",
    "                text=reader.read()\n",
    "            text=tokenize(text)\n",
    "            text=stop_words_removal(text)\n",
    "            text=reg_expressions(text)\n",
    "            data.append(text)\n",
    "            sentiments.append(sentiment)\n",
    "    data_np=np.array(data)\n",
    "    data, sentiments=unison_shuffle_data(data_np,sentiments)\n",
    "    \n",
    "    return data, sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsamp=1000\n",
    "maxtokens=50\n",
    "maxtokenlen=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def tokenize(row):\n",
    "    if row in [None,'']:\n",
    "        tokens=\"\"\n",
    "    else:\n",
    "        tokens=str(row).split(\" \")[:maxtokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def reg_expressions(row):\n",
    "    tokens=[]\n",
    "    try:\n",
    "        for token in row:\n",
    "            token=token.lower()\n",
    "            token=re.sub(r'[\\W\\d]', \"\", token)\n",
    "            token=token[:maxtokenlen]\n",
    "            tokens.append(token)\n",
    "    except:\n",
    "        token=\"\"\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopword: Package 'stopword' not found in\n",
      "[nltk_data]     index\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopword')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "def stop_words_removal(row):\n",
    "    token = [token for token in row if token not in stopwords]\n",
    "    token = filter(None, token)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_bag(data):\n",
    "    used_tokens=[]\n",
    "    all_tokens=[]\n",
    "    \n",
    "    for item in data:\n",
    "        for token in item:\n",
    "            if token in all_tokens:\n",
    "                if token not in used_tokens:\n",
    "                    used_tokens.append(token)\n",
    "            else:\n",
    "                all_tokens.append(token)\n",
    "    df=pd.DataFrame(0, index=np.arange(len(data)), columns=used_tokens)\n",
    "    \n",
    "    for i, item in enumerate(data):\n",
    "        for token in item:\n",
    "            if token in used_tokens:\n",
    "                df.iloc[i][token] += 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffle_data(data,header):\n",
    "    p=np.random.permutation(len(header))\n",
    "    data=data[p]\n",
    "    header=np.asarray(header)[p]\n",
    "    return data,header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\acham\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join('aclImdb','train')\n",
    "raw_data, raw_header = load_data(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(raw_data.shape)\n",
    "print(len(raw_header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indices = np.random.choice(range(len(raw_header)), size=(Nsamp*2,), replace=False)\n",
    "data_train = raw_data[random_indices]\n",
    "header = raw_header[random_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiments abd their frequencies:\n",
      "[0 1]\n",
      "[1022  978]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(header, return_counts=True)\n",
    "print(\"Sentiments abd their frequencies:\")\n",
    "print(unique_elements)\n",
    "print(counts_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      br  series  time  someone  i  one  boring  great  movie  play  ...  \\\n",
      "0      2       2     1        0  0    0       0      0      0     0  ...   \n",
      "1      0       0     1        2  0    0       0      1      1     0  ...   \n",
      "2      0       0     0        0  2    2       3      1      1     0  ...   \n",
      "3      0       0     0        0  0    1       0      0      0     2  ...   \n",
      "4      0       0     1        0  0    0       0      0      0     0  ...   \n",
      "...   ..     ...   ...      ... ..  ...     ...    ...    ...   ...  ...   \n",
      "1995   1       0     0        0  0    0       0      0      1     0  ...   \n",
      "1996   0       0     0        0  2    0       0      0      0     0  ...   \n",
      "1997   0       0     0        0  1    0       0      0      0     0  ...   \n",
      "1998   1       0     0        0  0    0       1      0      1     0  ...   \n",
      "1999   0       0     0        0  3    0       0      0      0     0  ...   \n",
      "\n",
      "      genuine  vader  melissa  lighthearted  undertones  atrocity  bridge  \\\n",
      "0           0      0        0             0           0         0       0   \n",
      "1           0      0        0             0           0         0       0   \n",
      "2           0      0        0             0           0         0       0   \n",
      "3           0      0        0             0           0         0       0   \n",
      "4           0      0        0             0           0         0       0   \n",
      "...       ...    ...      ...           ...         ...       ...     ...   \n",
      "1995        0      0        0             0           1         0       0   \n",
      "1996        0      0        0             0           0         1       0   \n",
      "1997        0      0        0             0           0         0       1   \n",
      "1998        0      0        0             0           0         0       0   \n",
      "1999        0      0        0             0           0         0       0   \n",
      "\n",
      "      gap  causes  loyalties  \n",
      "0       0       0          0  \n",
      "1       0       0          0  \n",
      "2       0       0          0  \n",
      "3       0       0          0  \n",
      "4       0       0          0  \n",
      "...   ...     ...        ...  \n",
      "1995    0       0          0  \n",
      "1996    0       0          0  \n",
      "1997    1       1          1  \n",
      "1998    0       0          0  \n",
      "1999    0       0          0  \n",
      "\n",
      "[2000 rows x 5053 columns]\n"
     ]
    }
   ],
   "source": [
    "MixedBagOfReviews=assemble_bag(data_train)\n",
    "print(MixedBagOfReviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(raw_data,header):\n",
    "    converted_data, labels = [], []\n",
    "    for i in range(raw_data.shape[0]):\n",
    "        # combine list of tokens representing each email into single string\n",
    "        out = ' '.join(raw_data[i])\n",
    "        converted_data.append(out)\n",
    "        labels.append(header[i])\n",
    "    converted_data = np.array(converted_data, dtype=object)[:, np.newaxis]\n",
    "    \n",
    "    return converted_data, np.array(labels)\n",
    "\n",
    "\n",
    "data = MixedBagOfReviews.values\n",
    "\n",
    "idx = int(0.7*data.shape[0])\n",
    "\n",
    "# 70% of data for training\n",
    "train_x = data[:idx,:]\n",
    "train_y = header[:idx]\n",
    "# remaining 30% for testing\n",
    "test_x = data[idx:,:]\n",
    "test_y = header[idx:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def fit(train_x, train_y):\n",
    "    model=LogisticRegression()\n",
    "    \n",
    "    try:\n",
    "        model.fit(train_x,train_y)\n",
    "    except:\n",
    "        pass\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Logistic Regression accuracy score is::\n",
      "0.69\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = model.predict(test_x)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_score = accuracy_score(test_y, predicted_labels)\n",
    "print(\"The Logistic Regression accuracy score is::\")\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf=SVC(C=1, gamma=\"auto\", kernel='linear', probability=False)\n",
    "start_time = time.time()\n",
    "clf.fit(train_x,train_y)\n",
    "end_time=time.time()\n",
    "print(\"Training the SVC Classifier took %3d seconds\"testing accuracy score is::\")\n",
    "predicted_labels=clf.predict(test_x)\n",
    "acc_score=accuracy_score(test_y, predicted_labels)\n",
    "print(\"The SVC Classifier testing accuracy score is::\")\n",
    "print(acc_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
